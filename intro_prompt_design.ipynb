{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kkvalsvik/LocalDataChatbot/blob/master/intro_prompt_design.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ur8xi4C7S06n"
      },
      "outputs": [],
      "source": [
        "# Copyright 2023 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAPoU8Sm5E6e"
      },
      "source": [
        "# Prompt Design - Best Practices\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/language/prompts/intro_prompt_design.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\"><br> Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/language/prompts/intro_prompt_design.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/blob/main/language/prompts/intro_prompt_design.ipynb\">\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvgnzT1CKxrO"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This notebook covers the essentials of prompt engineering, including some best practices.\n",
        "\n",
        "Learn more about prompt design in the [official documentation](https://cloud.google.com/vertex-ai/docs/generative-ai/text/text-overview)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d975e698c9a4"
      },
      "source": [
        "### Objective\n",
        "\n",
        "In this notebook, you learn best practices around prompt engineering -- how to design prompts to improve the quality of your responses.\n",
        "\n",
        "This notebook covers the following best practices for prompt engineering:\n",
        "\n",
        "- Be concise\n",
        "- Be specific and well-defined\n",
        "- Ask one task at a time\n",
        "- Turn generative tasks into classification tasks\n",
        "- Improve response quality by including examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ea013f50403c"
      },
      "source": [
        "### Costs\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI Generative AI Studio\n",
        "\n",
        "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing),\n",
        "and use the [Pricing Calculator](https://cloud.google.com/products/calculator/)\n",
        "to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3e663cb43fa0"
      },
      "source": [
        "### Install Vertex AI SDK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82ad0c445061",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71b09010-f754-4862-e987-4e49b872e8ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: shapely<2.0.0 in /usr/local/lib/python3.10/dist-packages (1.8.5.post1)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (0.27.9)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.7.22)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install \"shapely<2.0.0\"\n",
        "!pip install openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cebd6983cbad"
      },
      "source": [
        "**Colab only:** Uncomment the following cell to restart the kernel or use the button to restart the kernel. For Vertex AI Workbench you can restart the terminal using the button on top."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bea801acf6b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc6feba9-fbe9-49af-b2fb-30535f3ab861"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'status': 'ok', 'restart': True}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# Automatically restart kernel after installs so that your environment can access the new packages\n",
        "import IPython\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a386d25fa8f"
      },
      "source": [
        "### Authenticating your notebook environment\n",
        "* If you are using **Colab** to run this notebook, uncomment the cell below and continue.\n",
        "* If you are using **Vertex AI Workbench**, check out the setup instructions [here](https://github.com/GoogleCloudPlatform/generative-ai/tree/main/setup-env)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1bd1dca8e9a7"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "960505627ddf"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pq4zCFqk3S5F"
      },
      "source": [
        "**Colab only:** Uncomment the following cell to initialize the Vertex AI SDK. For Vertex AI Workbench, you don't need to run this.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WdKCmoiw3S5F"
      },
      "outputs": [],
      "source": [
        "# import vertexai\n",
        "\n",
        "# PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
        "# vertexai.init(project=PROJECT_ID, location=\"us-central1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PyQmSRbKA8r-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UP76a2la7O-a"
      },
      "source": [
        "### Load model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7isig7e07O-a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f63cd33-bdd3-4d54-ac09-fdcd1cd71f97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dear [Friend's Name],\n",
            "\n",
            "I hope this letter finds you well and in good spirits. It has been far too long since we last caught up, and I want you to know that I often find myself reminiscing about our shared memories and the meaningful conversations we used to have. It is in this spirit that I felt compelled to write to you and rekindle our bond, even if only through words.\n",
            "\n",
            "I have so much to tell you about the happenings in my life since we last spoke. Firstly, I wanted to share some exciting newsâ€”I recently got promoted at work! It was a significant milestone for me and has opened up new avenues for personal and professional growth. However, with this promotion came an increase in responsibilities, which has kept me quite occupied. Nonetheless, I'm grateful for the opportunity and the chance to challenge myself.\n",
            "\n",
            "In terms of my personal life, I have been fortunate enough to cultivate some fulfilling relationships and make wonderful new friends. There is something truly special about meeting people who share your values and interests. It brings a refreshing perspective to one's life and broadens horizons. I have also been meeting old acquaintances and rekindling friendships that have been dormant for too long, and reconnecting with you through this letter is a testament to that.\n",
            "\n",
            "Speaking of friendships, I often find myself reminiscing about the adventures we embarked on together. Remember when we used to go hiking every weekend? Those memories are etched in my mind and continue to bring a smile to my face. Although life has become busier for both of us, it would be wonderful to plan a reunion and relive those incredible moments. Maybe we can plan a trip or a simple get-together to catch up and create new memories.\n",
            "\n",
            "In terms of hobbies and interests, I have been exploring new passions and trying my hand at various activities. Recently, I started painting and photography as a form of expression and relaxation. It has been therapeutic and rejuvenating for my soul. What about you? Have you discovered any new interests or pursued any long-time passions? I would be thrilled to hear about them.\n",
            "\n",
            "Lastly, I couldn't help but reflect on the state of the world in light of the recent events and challenges we have collectively faced. The COVID-19 pandemic has brought about unprecedented changes and has impacted lives in various ways. It reminds us of the importance of cherishing our connections and staying resilient through adversity. I sincerely hope you and your loved ones are safe and healthy during these trying times.\n",
            "\n",
            "I miss our conversations, the laughter we shared, and the understanding we had. Let's not let more time pass before we reconnect in person or at least through more frequent communication. Life moves swiftly, and I believe it is essential to nurture the relationships that bring us joy and warmth.\n",
            "\n",
            "Please write back at your convenience and share your thoughts, experiences, and any updates from your life. I look forward to hearing from you and renewing our friendship.\n",
            "\n",
            "Sending warm wishes and heartfelt regards,\n",
            "\n",
            "[Your Name]\n"
          ]
        }
      ],
      "source": [
        "import openai\n",
        "\n",
        "openai.api_key = \"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\"\n",
        "model_name = \"gpt-3.5-turbo-0613\"\n",
        "#openaiRequest = model=model_name,\n",
        "#    messages = [{\n",
        "#        \"role\": \"system\",\n",
        "#        \"content\": \"\\n\\nHello there, how may I assist you today?\",\n",
        "#        },\n",
        "#         {\n",
        "#            \"role\": \"user\",\n",
        "#            \"content\": \"Your task is to generate an informative letter to an old friend\"\n",
        "#            }]\n",
        "prompt = \"Your task is to generate an informative letter to an old friend\"\n",
        "response = openai.ChatCompletion.create(\n",
        "    model=model_name,\n",
        "    messages=[{\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"\\n\\nHello there, how may I assist you today?\",\n",
        "        },\n",
        "         {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": prompt\n",
        "            }]\n",
        ")\n",
        "print(response[\"choices\"][0][\"message\"][\"content\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIPcn5dZ7O-b"
      },
      "source": [
        "## Prompt engineering best practices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df7d153f4928"
      },
      "source": [
        "Prompt engineering is all about how to design your prompts so that the response is what you were indeed hoping to see.\n",
        "\n",
        "The idea of using \"unfancy\" prompts is to minimize the noise in your prompt to reduce the possibility of the LLM misinterpreting the intent of the prompt. Below are a few guidelines on how to engineer \"unfancy\" prompts.\n",
        "\n",
        "In this section, you'll cover the following best practices when engineering prompts:\n",
        "\n",
        "* Be concise\n",
        "* Be specific, and well-defined\n",
        "* Ask one task at a time\n",
        "* Improve response quality by including examples\n",
        "* Turn generative tasks to classification tasks to improve safety"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43c1169ac435"
      },
      "source": [
        "### Be concise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0f380f1620e"
      },
      "source": [
        "ðŸ›‘ Not recommended. The prompt below is unnecessarily verbose."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6a1697c3603",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4fa5e87-2fd9-4953-8bc0-ec934ca73a1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Certainly! Here are a few suggestions for a name for your flower shop that specializes in dried flower bouquets:\n",
            "\n",
            "1. \"Everlast Blossoms\"\n",
            "2. \"The Dried Petal\"\n",
            "3. \"Eternal Blooms\"\n",
            "4. \"Serene Stems\"\n",
            "5. \"The Withered Rose\"\n",
            "6. \"Rustic Bouquets\"\n",
            "7. \"Fleur-de-Preserve\"\n",
            "8. \"The Dried Bloom Co.\"\n",
            "9. \"Vintage Violets\"\n",
            "10. \"Bouquets of Thyme\"\n",
            "\n",
            "I hope one of these names resonates with your vision for the shop. Let me know if you need any further assistance!\n"
          ]
        }
      ],
      "source": [
        "prompt = \"What do you think could be a good name for a flower shop that specializes in selling bouquets of dried flowers more than fresh flowers? Thank you!\"\n",
        "response = openai.ChatCompletion.create(\n",
        "    model=model_name,\n",
        "    messages=[{\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"\\n\\nHello there, how may I assist you today?\",\n",
        "        },\n",
        "         {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": prompt\n",
        "            }],\n",
        "    temperature=0\n",
        ")\n",
        "response1 = openai.ChatCompletion.create(\n",
        "    model=model_name,\n",
        "    messages=[{\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"\\n\\nHello there, how may I assist you today?\",\n",
        "        },\n",
        "         {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": prompt\n",
        "            }],\n",
        "    temperature=1 #kristian\n",
        ")\n",
        "print(response1[\"choices\"][0][\"message\"][\"content\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response[\"choices\"][0][\"message\"][\"content\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CM1rRUurABfo",
        "outputId": "600d5eb1-75f9-4b5d-9695-6ca682402777"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Certainly! Here are a few suggestions for a flower shop that specializes in dried flower bouquets:\n",
            "\n",
            "1. \"Everlast Blooms\"\n",
            "2. \"Petals & Preserves\"\n",
            "3. \"The Dried Bouquet\"\n",
            "4. \"Bloom & Dry\"\n",
            "5. \"Fleur de SÃ©chage\" (French for \"Flower Drying\")\n",
            "6. \"The Dried Petal\"\n",
            "7. \"Eternal Blossoms\"\n",
            "8. \"The Dried Garden\"\n",
            "9. \"Bouquet de SÃ©chage\" (French for \"Dried Bouquet\")\n",
            "10. \"The Fragrant Keepsake\"\n",
            "\n",
            "I hope one of these names resonates with you! Let me know if you need any further assistance.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2307f56a9b75"
      },
      "source": [
        "âœ… Recommended. The prompt below is to the point and concise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fc666404f47c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d909bde8-ba41-41bd-8658-311b4281ff56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "How about \"Everlast Blooms\"? It reflects the concept of your flower shop selling bouquets of dried flowers that can last for a longer period of time.\n"
          ]
        }
      ],
      "source": [
        "prompt = \"Suggest a name for a flower shop that sells bouquets of dried flowers\"\n",
        "response = openai.ChatCompletion.create(\n",
        "    model=model_name,\n",
        "    messages=[{\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"\\n\\nHello there, how may I assist you today?\",\n",
        "        },\n",
        "         {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": prompt\n",
        "            }]\n",
        ")\n",
        "print(response[\"choices\"][0][\"message\"][\"content\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17f6c48bba91"
      },
      "source": [
        "### Be specific, and well-defined"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "269b428e1563"
      },
      "source": [
        "Suppose that you want to brainstorm creative ways to describe Earth."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6436ee2ff406"
      },
      "source": [
        "ðŸ›‘ Not recommended. The prompt below is too generic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "261b7f6e94c5"
      },
      "outputs": [],
      "source": [
        "prompt = \"Tell me about Earth\"\n",
        "\n",
        "print(generation_model.predict(prompt=prompt, max_output_tokens=256).text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bebfecd2912"
      },
      "source": [
        "âœ… Recommended. The prompt below is specific and well-defined."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "242b1b3bae6e"
      },
      "outputs": [],
      "source": [
        "prompt = \"Generate a list of ways that makes Earth unique compared to other planets\"\n",
        "\n",
        "print(generation_model.predict(prompt=prompt, max_output_tokens=256).text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20dca9a05eab"
      },
      "source": [
        "### Ask one task at a time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9019d443179"
      },
      "source": [
        "ðŸ›‘ Not recommended. The prompt below has two parts to the question that could be asked separately."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70b3b5e5825d"
      },
      "outputs": [],
      "source": [
        "prompt = \"What's the best method of boiling water and why is the sky blue?\"\n",
        "\n",
        "print(generation_model.predict(prompt=prompt, max_output_tokens=256).text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7936fb58c16a"
      },
      "source": [
        "âœ… Recommended. The prompts below asks one task a time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2564dad6c8db"
      },
      "outputs": [],
      "source": [
        "prompt = \"What's the best method of boiling water?\"\n",
        "\n",
        "print(generation_model.predict(prompt=prompt, max_output_tokens=256).text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "770c695ade92"
      },
      "outputs": [],
      "source": [
        "prompt = \"Why is the sky blue?\"\n",
        "\n",
        "print(generation_model.predict(prompt=prompt, max_output_tokens=256).text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ff606011aa86"
      },
      "source": [
        "### Watch out for hallucinations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "956ce45b06a7"
      },
      "source": [
        "Although LLMs have been trained on a large amount of data, they can generate text containing statements not grounded in truth or reality; these responses from the LLM are often referred to as \"hallucinations\" due to their limited memorization capabilities. Note that simply prompting the LLM to provide a citation isnâ€™t a fix to this problem, as there are instances of LLMs providing false or inaccurate citations. Dealing with hallucinations is a fundamental challenge of LLMs and an ongoing research area, so it is important to be cognizant that LLMs may seem to give you confident, correct-sounding statements that are in fact incorrect.\n",
        "\n",
        "Note that if you intend to use LLMs for the creative use cases, hallucinating could actually be quite useful."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c9d5f66179a"
      },
      "source": [
        "Try the prompt like the one below repeatedly. You may notice that sometimes it will confidently, but inaccurately, say \"The first elephant to visit the moon was Luna\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d813b9061b08"
      },
      "outputs": [],
      "source": [
        "prompt = \"Who was the first elephant to visit the moon?\"\n",
        "\n",
        "print(generation_model.predict(prompt=prompt, max_output_tokens=256).text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "029e23abfd56"
      },
      "source": [
        "### Turn generative tasks into classification tasks to reduce output variability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d943941d6e59"
      },
      "source": [
        "#### Generative tasks lead to higher output variability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37528e6c9754"
      },
      "source": [
        "The prompt below results in an open-ended response, useful for brainstorming, but response is highly variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a8e2dc39e9ae"
      },
      "outputs": [],
      "source": [
        "prompt = \"I'm a high school student. Recommend me a programming activity to improve my skills.\"\n",
        "\n",
        "print(generation_model.predict(prompt=prompt, max_output_tokens=256).text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f71a6fa2b4bb"
      },
      "source": [
        "#### Classification tasks reduces output variability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "917517465dac"
      },
      "source": [
        "The prompt below results in a choice and may be useful if you want the output to be easier to control."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3feb93d9df81"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"I'm a high school student. Which of these activities do you suggest and why:\n",
        "a) learn Python\n",
        "b) learn Javascript\n",
        "c) learn Fortran\n",
        "\"\"\"\n",
        "\n",
        "print(generation_model.predict(prompt=prompt, max_output_tokens=256).text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32290ac9fb2b"
      },
      "source": [
        "### Improve response quality by including examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "132834f5db2c"
      },
      "source": [
        "Another way to improve response quality is to add examples in your prompt. The LLM learns in-context from the examples on how to respond. Typically, one to five examples (shots) are enough to improve the quality of responses. Including too many examples can cause the model to over-fit the data and reduce the quality of responses.\n",
        "\n",
        "Similar to classical model training, the quality and distribution of the examples is very important. Pick examples that are representative of the scenarios that you need the model to learn, and keep the distribution of the examples (e.g. number of examples per class in the case of classification) aligned with your actual distribution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46520d938b6a"
      },
      "source": [
        "#### Zero-shot prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46d3b47e6cea"
      },
      "source": [
        "Below is an example of zero-shot prompting, where you don't provide any examples to the LLM within the prompt itself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2cbe03eb0b71"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"Decide whether a Tweet's sentiment is positive, neutral, or negative.\n",
        "\n",
        "Tweet: I loved the new YouTube video you made!\n",
        "Sentiment:\n",
        "\"\"\"\n",
        "\n",
        "print(generation_model.predict(prompt=prompt, max_output_tokens=256).text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0daabca1359"
      },
      "source": [
        "#### One-shot prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42c4652fc5c2"
      },
      "source": [
        "Below is an example of one-shot prompting, where you provide one example to the LLM within the prompt to give some guidance on what type of response you want."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cfe584860787"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"Decide whether a Tweet's sentiment is positive, neutral, or negative.\n",
        "\n",
        "Tweet: I loved the new YouTube video you made!\n",
        "Sentiment: positive\n",
        "\n",
        "Tweet: That was awful. Super boring ðŸ˜ \n",
        "Sentiment:\n",
        "\"\"\"\n",
        "\n",
        "print(generation_model.predict(prompt=prompt, max_output_tokens=256).text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef58c35005c0"
      },
      "source": [
        "#### Few-shot prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b630e8947b60"
      },
      "source": [
        "Below is an example of few-shot prompting, where you provide one example to the LLM within the prompt to give some guidance on what type of response you want."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fb3ba21bbd11"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"Decide whether a Tweet's sentiment is positive, neutral, or negative.\n",
        "\n",
        "Tweet: I loved the new YouTube video you made!\n",
        "Sentiment: positive\n",
        "\n",
        "Tweet: That was awful. Super boring ðŸ˜ \n",
        "Sentiment: negative\n",
        "\n",
        "Tweet: Something surprised me about this video - it was actually original. It was not the same old recycled stuff that I always see. Watch it - you will not regret it.\n",
        "Sentiment:\n",
        "\"\"\"\n",
        "\n",
        "print(generation_model.predict(prompt=prompt, max_output_tokens=256).text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4023be726eb"
      },
      "source": [
        "#### Choosing between zero-shot, one-shot, few-shot prompting methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6d7870ff75cc"
      },
      "source": [
        "Which prompt technique to use will solely depends on your goal. The zero-shot prompts are more open-ended and can give you creative answers, while one-shot and few-shot prompts teach the model how to behave so you can get more predictable answers that are consistent with the examples provided."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "environment": {
      "kernel": "python3",
      "name": "tf2-gpu.2-11.m108",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-11:m108"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}